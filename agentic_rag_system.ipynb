{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG System with Azure OpenAI & LangGraph\n",
    "\n",
    "This Jupyter Notebook implements an Agentic Retrieval-Augmented Generation (RAG) system, incorporating Pinecone serverless setup, Azure OpenAI for embeddings and answer generation, Google Gemini for self-critique, and LangGraph for workflow orchestration. It includes MLflow for observability with prompt registration and evaluation metrics, addressing the `GraphRecursionError` by limiting refinements to one iteration.\n",
    "\n",
    "**Components**:\n",
    "- **Azure OpenAI**: `text-embedding-3-small` for embeddings, `gpt4o` for answer generation.\n",
    "- **Pinecone**: Serverless vector database for storing and retrieving embeddings.\n",
    "- **Google Gemini**: `gemini-2.5-flash` for self-critique.\n",
    "- **LangGraph**: Orchestrates four nodes (Retriever, LLM Answer, Self-Critique, Refinement).\n",
    "- **MLflow**: Logs queries, snippets, answers, critiques, refinement status, and evaluation metrics (latency, relevance, readability).\n",
    "\n",
    "**Functionality**:\n",
    "- Processes `self_critique_loop_dataset.json` (30 entries).\n",
    "- Retrieves up to 5 knowledge base snippets, generates answers with `[KBxxx]` citations.\n",
    "- Performs one self-critique and, if needed, one refinement with an additional snippet (top-6).\n",
    "- Logs results and evaluates answers using MLflow.\n",
    "\n",
    "**Sample Queries**:\n",
    "- What are best practices for caching?\n",
    "- How should I set up CI/CD pipelines?\n",
    "- What are performance tuning tips?\n",
    "- How do I version my APIs?\n",
    "- What should I consider for error handling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~lflow (/home/zadmin/Desktop/GAAI-B4-Azure/genai/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~lflow (/home/zadmin/Desktop/GAAI-B4-Azure/genai/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~lflow (/home/zadmin/Desktop/GAAI-B4-Azure/genai/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~lflow (/home/zadmin/Desktop/GAAI-B4-Azure/genai/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~lflow (/home/zadmin/Desktop/GAAI-B4-Azure/genai/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~lflow (/home/zadmin/Desktop/GAAI-B4-Azure/genai/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~lflow (/home/zadmin/Desktop/GAAI-B4-Azure/genai/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~lflow (/home/zadmin/Desktop/GAAI-B4-Azure/genai/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~lflow (/home/zadmin/Desktop/GAAI-B4-Azure/genai/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~lflow (/home/zadmin/Desktop/GAAI-B4-Azure/genai/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~lflow (/home/zadmin/Desktop/GAAI-B4-Azure/genai/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~lflow (/home/zadmin/Desktop/GAAI-B4-Azure/genai/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install langgraph langchain-openai pinecone mlflow pydantic google-generativeai openai dspy litellm textstat evaluate --quiet\n",
    "!python -m pip install --upgrade mlflow --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from openai import AzureOpenAI\n",
    "import google.generativeai as genai\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Any\n",
    "import mlflow\n",
    "from mlflow import register_prompt\n",
    "print(mlflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINECONE API KEY found\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set MLflow tracking URI\n",
    "mlflow.set_tracking_uri(\"http://20.75.92.162:5000/\")\n",
    "\n",
    "embedding_model_name = \"text-embedding-3-small\"\n",
    "embedding_deployment_name = \"text-embedding-3-small\"  # Replace with your Azure deployment name\n",
    "\n",
    "# Verify environment variables\n",
    "assert os.environ.get(\"AZURE_OPENAI_ENDPOINT\"), \"AZURE_OPENAI_ENDPOINT not set in .env\"\n",
    "assert os.environ.get(\"AZURE_OPENAI_API_KEY\"), \"AZURE_OPENAI_API_KEY not set in .env\"\n",
    "assert os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2024-06-01\"), \"AZURE_OPENAI_API_VERSION not set in .env\"\n",
    "assert os.environ.get(\"PINECONE_API_KEY\"), \"PINECONE_API_KEY not set in .env\"\n",
    "assert os.environ.get(\"GOOGLE_API_KEY\"), \"GOOGLE_API_KEY not set in .env\"\n",
    "\n",
    "# Verify Pinecone API key\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "if not PINECONE_API_KEY:\n",
    "    raise RuntimeError(\"PINECONE_API_KEY not found. Please set it in your environment.\")\n",
    "else:\n",
    "    print(\"PINECONE API KEY found\")\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing & Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load the Knowledge Base JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30 entries from the knowledge base.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "with open('self_critique_loop_dataset.json', 'r') as f:\n",
    "    kb_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(kb_data)} entries from the knowledge base.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Set Up Azure OpenAI Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 1536\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://20.75.92.162:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-fdf018b7ffc53aa37b60ef459a6394e9&amp;experiment_id=218609737045096119&amp;version=3.4.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-fdf018b7ffc53aa37b60ef459a6394e9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=embedding_model_name,\n",
    "    deployment=embedding_deployment_name,\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    ")\n",
    "\n",
    "# Detect embedding dimension\n",
    "test_dim = len(embeddings.embed_query(\"dimension probe\"))\n",
    "print(f\"Embedding dimension: {test_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Create and Populate Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'agentic-rag-kb' already exists, reusing it.\n",
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'': {'vector_count': 30}},\n",
      " 'total_vector_count': 30,\n",
      " 'vector_type': 'dense'}\n",
      "Upsert complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://20.75.92.162:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-aed01edcb514fa60f66a085c1f47a7bf&amp;experiment_id=218609737045096119&amp;trace_id=tr-67727763345b256563815874ee05484f&amp;experiment_id=218609737045096119&amp;trace_id=tr-dece1a847f4e8373d47dac2dec191acb&amp;experiment_id=218609737045096119&amp;trace_id=tr-5fd4b9d72b47d7dbd0eaa799728dc4b1&amp;experiment_id=218609737045096119&amp;trace_id=tr-aa4babaaf4bdf8577c7114569ef66e31&amp;experiment_id=218609737045096119&amp;trace_id=tr-f700fc081132ed65fbc395163d7bfba7&amp;experiment_id=218609737045096119&amp;trace_id=tr-cc1e0ada647709a962ee832d84a0eb62&amp;experiment_id=218609737045096119&amp;trace_id=tr-cbf34b3ba78ab58a97a643ae0641982b&amp;experiment_id=218609737045096119&amp;trace_id=tr-0b88b59680923ad44a9b2f6a4ee806da&amp;experiment_id=218609737045096119&amp;trace_id=tr-9fe93eb93740fefc722592b14b33e5d0&amp;experiment_id=218609737045096119&amp;version=3.4.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-aed01edcb514fa60f66a085c1f47a7bf), Trace(trace_id=tr-67727763345b256563815874ee05484f), Trace(trace_id=tr-dece1a847f4e8373d47dac2dec191acb), Trace(trace_id=tr-5fd4b9d72b47d7dbd0eaa799728dc4b1), Trace(trace_id=tr-aa4babaaf4bdf8577c7114569ef66e31), Trace(trace_id=tr-f700fc081132ed65fbc395163d7bfba7), Trace(trace_id=tr-cc1e0ada647709a962ee832d84a0eb62), Trace(trace_id=tr-cbf34b3ba78ab58a97a643ae0641982b), Trace(trace_id=tr-0b88b59680923ad44a9b2f6a4ee806da), Trace(trace_id=tr-9fe93eb93740fefc722592b14b33e5d0)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define index parameters\n",
    "INDEX_NAME = \"agentic-rag-kb\"\n",
    "METRIC = \"cosine\"\n",
    "\n",
    "# Create index if it doesn't exist\n",
    "existing = [idx[\"name\"] for idx in pc.list_indexes()]\n",
    "if INDEX_NAME not in existing:\n",
    "    print(f\"Creating index '{INDEX_NAME}' ...\")\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=test_dim,\n",
    "        metric=METRIC,\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "    time.sleep(5)\n",
    "else:\n",
    "    print(f\"Index '{INDEX_NAME}' already exists, reusing it.\")\n",
    "\n",
    "index = pc.Index(INDEX_NAME)\n",
    "print(index.describe_index_stats())\n",
    "\n",
    "# Prepare vectors: id, vector, metadata\n",
    "vectors = []\n",
    "for entry in kb_data:\n",
    "    text_to_embed = f\"{entry['question']} {entry['answer_snippet']}\"\n",
    "    vector = embeddings.embed_query(text_to_embed)\n",
    "    vectors.append({\n",
    "        \"id\": entry['doc_id'],\n",
    "        \"values\": vector,\n",
    "        \"metadata\": {\n",
    "            \"question\": entry['question'],\n",
    "            \"text\": entry['answer_snippet'],\n",
    "            \"source\": entry['source'],\n",
    "            \"confidence_indicator\": entry['confidence_indicator'],\n",
    "            \"last_updated\": entry['last_updated']\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Upsert vectors to Pinecone\n",
    "index.upsert(vectors=vectors)\n",
    "print(\"Upsert complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LangGraph Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Set Up Azure OpenAI LLM (GPT-4o-mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI client\n",
    "azure_client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "def generate_answer(query, snippets, temperature=0):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant. Generate a concise answer based on the provided snippets. Cite them as [KBxxx]. Ensure the answer directly addresses the query.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Query: {query}\\nSnippets:\\n\" + \"\\n\".join(\n",
    "                [f\"[{s['id']}] {s['metadata']['question']} {s['metadata']['text']}\" for s in snippets]\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    response = azure_client.chat.completions.create(\n",
    "        model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Set Up Gemini for Self-Critique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini API\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "critique_model = \"gemini-2.5-flash\"\n",
    "\n",
    "def self_critique(query, answer):\n",
    "    prompt = f\"Query: {query}\\nAnswer: {answer}\\nIs this answer COMPLETE or does it need REFINE? Output only 'COMPLETE' or 'REFINE'.\"\n",
    "    response = genai.GenerativeModel(critique_model).generate_content(prompt)\n",
    "    return response.text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_snippets(query, k=5):\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "    results = index.query(vector=query_vector, top_k=k, include_metadata=True)\n",
    "    return results['matches']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Define LangGraph Nodes and State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state\n",
    "class State(TypedDict):\n",
    "    query: str\n",
    "    snippets: List[Any]\n",
    "    answer: str\n",
    "    critique: str\n",
    "    refined: bool\n",
    "    refinement_count: int  # Track number of refinements\n",
    "\n",
    "# Node functions\n",
    "def retriever_node(state: State) -> State:\n",
    "    state['snippets'] = retrieve_snippets(state['query'], k=5)\n",
    "    state['refinement_count'] = state.get('refinement_count', 0)  # Initialize counter\n",
    "    return state\n",
    "\n",
    "def llm_answer_node(state: State) -> State:\n",
    "    snippets = state['snippets'][:5] if not state.get('refined', False) else state['snippets'][:6]\n",
    "    state['answer'] = generate_answer(state['query'], snippets)\n",
    "    return state\n",
    "\n",
    "def self_critique_node(state: State) -> State:\n",
    "    state['critique'] = self_critique(state['query'], state['answer'])\n",
    "    return state\n",
    "\n",
    "def refinement_node(state: State) -> State:\n",
    "    if state['critique'] == \"REFINE\" and state.get('refinement_count', 0) < 1:  # Limit to 1 refinement\n",
    "        state['snippets'] = retrieve_snippets(state['query'], k=6)\n",
    "        state['refined'] = True\n",
    "        state['refinement_count'] = state.get('refinement_count', 0) + 1\n",
    "    return state\n",
    "\n",
    "# Decision function for conditional edge\n",
    "def decide_to_refine(state: State):\n",
    "    if state['critique'] == \"COMPLETE\" or state.get('refinement_count', 0) >= 1:\n",
    "        return \"end\"\n",
    "    return \"refinement\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Build the LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the graph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"retriever\", retriever_node)\n",
    "workflow.add_node(\"llm_answer\", llm_answer_node)\n",
    "workflow.add_node(\"self_critique\", self_critique_node)\n",
    "workflow.add_node(\"refinement\", refinement_node)\n",
    "\n",
    "# Define edges\n",
    "workflow.set_entry_point(\"retriever\")\n",
    "workflow.add_edge(\"retriever\", \"llm_answer\")\n",
    "workflow.add_edge(\"llm_answer\", \"self_critique\")\n",
    "workflow.add_conditional_edges(\"self_critique\", decide_to_refine, {\"end\": END, \"refinement\": \"refinement\"})\n",
    "workflow.add_edge(\"refinement\", \"llm_answer\")\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tracing & Observability with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5751/803866029.py:7: FutureWarning: The `mlflow.register_prompt` API is moved to the `mlflow.genai` namespace. Please use `mlflow.genai.register_prompt` instead. The original API will be removed in the future release.\n",
      "  prompt = mlflow.register_prompt(\n",
      "2025/09/30 17:35:14 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: raj-agentic-rag-answer-prompt, version 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created prompt 'raj-agentic-rag-answer-prompt' (version 7)\n"
     ]
    }
   ],
   "source": [
    "# Register the prompt template\n",
    "answer_template = \"\"\"\\\n",
    "You are a helpful assistant. Generate a concise answer based on the provided snippets for the query: {{ query }}.\n",
    "Snippets: {{ snippets }}\n",
    "Cite snippets as [KBxxx].\n",
    "\"\"\"\n",
    "prompt = mlflow.register_prompt(\n",
    "    name=\"raj-agentic-rag-answer-prompt\",\n",
    "    template=answer_template,\n",
    "    commit_message=\"Initial commit for RAG answer prompt\",\n",
    ")\n",
    "print(f\"Created prompt '{prompt.name}' (version {prompt.version})\")\n",
    "\n",
    "# Enable autologging for Azure OpenAI\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "def run_query_with_logging(query):\n",
    "    # Prepare evaluation data\n",
    "    eval_data = pd.DataFrame({\n",
    "        \"inputs\": [query],\n",
    "        \"targets\": [\"\"]  # Placeholder; ground truth not available\n",
    "    })\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"Query: {query[:20]}...\"):\n",
    "        mlflow.log_param(\"query\", query)\n",
    "        mlflow.log_param(\"model\", \"gpt-4o-mini\")\n",
    "        mlflow.log_param(\"temperature\", 0)\n",
    "\n",
    "        # Run the workflow\n",
    "        result = app.invoke(\n",
    "            {\"query\": query, \"refined\": False, \"refinement_count\": 0},\n",
    "            config={\"recursion_limit\": 50}\n",
    "        )\n",
    "\n",
    "        # Log standard metrics\n",
    "        mlflow.log_metric(\"refined\", 1 if result.get('refined', False) else 0)\n",
    "        mlflow.log_text(result['answer'], \"final_answer.txt\")\n",
    "        mlflow.log_text(\"\\n\".join([s['id'] for s in result['snippets']]), \"retrieved_snippets.txt\")\n",
    "        mlflow.log_text(result['critique'], \"critique.txt\")\n",
    "        mlflow.log_text(\n",
    "            \"\\n\".join([f\"[{s['id']}] {s['metadata']['question']} {s['metadata']['text']}\" for s in result['snippets']]),\n",
    "            \"snippet_details.txt\"\n",
    "        )\n",
    "\n",
    "        # Evaluate the answer\n",
    "        def predict(data: pd.DataFrame) -> list[str]:\n",
    "            predictions = []\n",
    "            prompt_obj = mlflow.genai.load_prompt(f\"prompts:/raj-agentic-rag-answer-prompt/{prompt.version}\")\n",
    "            for _, row in data.iterrows():\n",
    "                snippets_text = \"\\n\".join(\n",
    "                    [f\"[{s['id']}] {s['metadata']['question']} {s['metadata']['text']}\" for s in result['snippets']]\n",
    "                )\n",
    "                content = prompt_obj.format(query=row[\"inputs\"], snippets=snippets_text)\n",
    "                completion = azure_client.chat.completions.create(\n",
    "                    model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "                    messages=[{\"role\": \"user\", \"content\": content}],\n",
    "                    temperature=0,\n",
    "                )\n",
    "                predictions.append(completion.choices[0].message.content)\n",
    "            return predictions\n",
    "\n",
    "        # Run MLflow evaluation\n",
    "        eval_results = mlflow.evaluate(\n",
    "            model=predict,\n",
    "            data=eval_data,\n",
    "            targets=\"targets\",\n",
    "            extra_metrics=[\n",
    "                mlflow.metrics.latency(),\n",
    "                mlflow.metrics.ari_grade_level(),\n",
    "                mlflow.metrics.flesch_kincaid_grade_level(),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Log evaluation results\n",
    "        for metric_name, metric_value in eval_results.metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test with Sample Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 17:35:42 INFO mlflow.models.evaluation.evaluators.default: Computing model predictions.\n",
      "2025/09/30 17:35:48 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " View run Query: What are best practi... at: http://20.75.92.162:5000/#/experiments/218609737045096119/runs/1977b70ccc0540f5bf16d9a332778a63\n",
      "離 View experiment at: http://20.75.92.162:5000/#/experiments/218609737045096119\n",
      "Query: What are best practices for caching?\n",
      "Answer: Best practices for caching include following well-defined patterns to ensure efficiency and effectiveness. This involves strategies such as setting appropriate expiration times, using cache keys wisely, and regularly monitoring cache performance to optimize resource usage and minimize latency [KB003][KB023][KB013].\n",
      "Snippets: ['KB003', 'KB023', 'KB013', 'KB002', 'KB012', 'KB022']\n",
      "Critique: REFINE\n",
      "Refined: True\n",
      "Refinement Count: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 17:36:44 INFO mlflow.models.evaluation.evaluators.default: Computing model predictions.\n",
      "2025/09/30 17:36:52 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " View run Query: How should I set up ... at: http://20.75.92.162:5000/#/experiments/218609737045096119/runs/731e01f0ba724361a4275186ccaefbc8\n",
      "離 View experiment at: http://20.75.92.162:5000/#/experiments/218609737045096119\n",
      "Query: How should I set up CI/CD pipelines?\n",
      "Answer: To set up CI/CD pipelines effectively, follow these best practices: \n",
      "\n",
      "1. **Define Clear Stages**: Structure your pipeline into distinct stages such as build, test, and deploy.\n",
      "2. **Automate Testing**: Integrate automated testing to ensure code quality at every stage.\n",
      "3. **Use Version Control**: Maintain your code in a version control system to track changes and facilitate collaboration.\n",
      "4. **Monitor and Optimize**: Continuously monitor the performance of your pipelines and optimize them for efficiency.\n",
      "5. **Implement Rollback Mechanisms**: Ensure you have a strategy for rolling back deployments in case of failures.\n",
      "\n",
      "These practices help in maintaining a robust and efficient CI/CD process [KB007][KB017][KB027].\n",
      "Snippets: ['KB007', 'KB017', 'KB027', 'KB016', 'KB006', 'KB026']\n",
      "Critique: REFINE\n",
      "Refined: True\n",
      "Refinement Count: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 17:37:51 INFO mlflow.models.evaluation.evaluators.default: Computing model predictions.\n",
      "2025/09/30 17:37:56 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " View run Query: What are performance... at: http://20.75.92.162:5000/#/experiments/218609737045096119/runs/1f837fac0512456e8e95271e78da8bbc\n",
      "離 View experiment at: http://20.75.92.162:5000/#/experiments/218609737045096119\n",
      "Query: What are performance tunning tips?\n",
      "Answer: For effective performance tuning, consider the following best practices: \n",
      "\n",
      "1. **Identify Bottlenecks**: Use profiling tools to find slow parts of your application.\n",
      "2. **Optimize Queries**: Ensure database queries are efficient and indexed properly.\n",
      "3. **Cache Results**: Implement caching strategies to reduce load times for frequently accessed data.\n",
      "4. **Monitor Resource Usage**: Keep an eye on CPU, memory, and disk I/O to identify resource constraints.\n",
      "5. **Use Asynchronous Processing**: Offload long-running tasks to background processes to improve responsiveness.\n",
      "6. **Review Code Efficiency**: Refactor code to eliminate unnecessary computations and improve algorithm efficiency.\n",
      "\n",
      "Following these patterns can lead to significant performance improvements [KB002][KB022][KB012].\n",
      "Snippets: ['KB002', 'KB022', 'KB012', 'KB011', 'KB001', 'KB021']\n",
      "Critique: REFINE\n",
      "Refined: True\n",
      "Refinement Count: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 17:38:49 INFO mlflow.models.evaluation.evaluators.default: Computing model predictions.\n",
      "2025/09/30 17:38:54 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " View run Query: How do i version my ... at: http://20.75.92.162:5000/#/experiments/218609737045096119/runs/2046ec5706ec43b7bbd644292f19a6aa\n",
      "離 View experiment at: http://20.75.92.162:5000/#/experiments/218609737045096119\n",
      "Query: How do i version my APIs?\n",
      "Answer: To version your APIs, it's important to follow well-defined patterns. Common practices include using version numbers in the URL (e.g., /v1/resource), in request headers, or as part of the query parameters. Ensure that each version is clearly documented and that you maintain backward compatibility where possible to avoid breaking changes for users of older versions [KB005][KB025][KB015].\n",
      "Snippets: ['KB005', 'KB025', 'KB015', 'KB007', 'KB027', 'KB017']\n",
      "Critique: REFINE\n",
      "Refined: True\n",
      "Refinement Count: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 17:39:46 INFO mlflow.models.evaluation.evaluators.default: Computing model predictions.\n",
      "2025/09/30 17:39:52 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " View run Query: What should i consid... at: http://20.75.92.162:5000/#/experiments/218609737045096119/runs/ed2a0397b4154526b54b42995c9891ce\n",
      "離 View experiment at: http://20.75.92.162:5000/#/experiments/218609737045096119\n",
      "Query: What should i consider for error handling?\n",
      "Answer: When considering error handling, you should focus on following well-defined patterns, ensuring that errors are logged appropriately, providing meaningful error messages, and implementing a strategy for recovery or fallback mechanisms. Additionally, consider the context of the application and the user experience when designing your error handling approach [KB009][KB029][KB019].\n",
      "Snippets: ['KB009', 'KB029', 'KB019', 'KB011', 'KB021', 'KB001']\n",
      "Critique: REFINE\n",
      "Refined: True\n",
      "Refinement Count: 1\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://20.75.92.162:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-5b78f218d564e625709ea6bb4d4f76e7&amp;experiment_id=218609737045096119&amp;trace_id=tr-d48825fe0ae196f0c6aa7c04c9d2f79e&amp;experiment_id=218609737045096119&amp;trace_id=tr-b3b3409e5f29e558d2cc54679180aaa1&amp;experiment_id=218609737045096119&amp;trace_id=tr-ad3655c01ce028649017cfc7a8d784cd&amp;experiment_id=218609737045096119&amp;trace_id=tr-6c2824c2f299020582663d5937169b15&amp;experiment_id=218609737045096119&amp;trace_id=tr-e11c787fccd474c3ea213c2af0864a65&amp;experiment_id=218609737045096119&amp;trace_id=tr-4442c6e204cafc2071e3f30d098b9f2e&amp;experiment_id=218609737045096119&amp;trace_id=tr-1649a0029238d623d0d1264a163e37b4&amp;experiment_id=218609737045096119&amp;trace_id=tr-0a39cff131afb2ea425923ab3f0f4a8c&amp;experiment_id=218609737045096119&amp;trace_id=tr-8d9d6d2435dbecfd3ff86736e8ec3ac9&amp;experiment_id=218609737045096119&amp;version=3.4.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-5b78f218d564e625709ea6bb4d4f76e7), Trace(trace_id=tr-d48825fe0ae196f0c6aa7c04c9d2f79e), Trace(trace_id=tr-b3b3409e5f29e558d2cc54679180aaa1), Trace(trace_id=tr-ad3655c01ce028649017cfc7a8d784cd), Trace(trace_id=tr-6c2824c2f299020582663d5937169b15), Trace(trace_id=tr-e11c787fccd474c3ea213c2af0864a65), Trace(trace_id=tr-4442c6e204cafc2071e3f30d098b9f2e), Trace(trace_id=tr-1649a0029238d623d0d1264a163e37b4), Trace(trace_id=tr-0a39cff131afb2ea425923ab3f0f4a8c), Trace(trace_id=tr-8d9d6d2435dbecfd3ff86736e8ec3ac9)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set MLflow experiment\n",
    "mlflow.set_experiment(\"raj-agentic-rag-evaluation\")\n",
    "\n",
    "sample_queries = [\n",
    "    \"What are best practices for caching?\",\n",
    "    \"How should I set up CI/CD pipelines?\",\n",
    "    \"What are performance tunning tips?\",\n",
    "    \"How do i version my APIs?\",\n",
    "    \"What should i consider for error handling?\"\n",
    "]\n",
    "\n",
    "for query in sample_queries:\n",
    "    result = run_query_with_logging(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Snippets: {[s['id'] for s in result['snippets']]}\")\n",
    "    print(f\"Critique: {result['critique']}\")\n",
    "    print(f\"Refined: {result.get('refined', False)}\")\n",
    "    print(f\"Refinement Count: {result.get('refinement_count', 0)}\")\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
